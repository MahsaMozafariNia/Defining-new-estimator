{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8bc4479",
   "metadata": {},
   "source": [
    "Prediction- Weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dfc9d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Item_Similarity_Computation import Similarity\n",
    "from datetime import datetime\n",
    "import operator\n",
    "import sklearn.metrics\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors as NN\n",
    "from Item_Similarity_Computation import Similarity\n",
    "from prediction_computation import prediction_compute\n",
    "from sklearn.metrics import r2_score \n",
    "\n",
    "#data,X,y,pivot=dataframe.dataset()\n",
    "\n",
    "class estimator(BaseEstimator,ClassifierMixin):\n",
    "    #BaseEstimator is for gerparam and set params\n",
    "    \n",
    "    def __init__(self,metric='cosine',n_neighbors=3,pred='weighted_sum'):\n",
    "        \n",
    "        self.n_neighbors=n_neighbors\n",
    "        self.metric=metric\n",
    "        self.my_metric=Similarity\n",
    "        self.pivot=None\n",
    "        self.dataset=None\n",
    "        self.similarity_matrix=None\n",
    "        self.X=None\n",
    "        self.y=None\n",
    "        self.pred=pred\n",
    "        self.my_predict=prediction_compute\n",
    "\n",
    "    '''\n",
    "    First, I am going to define knn using a metric in Item Similarity Computation.ipynb\n",
    "    \n",
    "    '''\n",
    "    #each estimator needs a fit, predict and score function.\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "     \n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.dataset=self.X.copy()\n",
    "        self.dataset['rating']=self.y\n",
    "        self.pivot=self.dataset.pivot_table(index=\"movieId\",columns=\"userId\",values=\"rating\").fillna(0)\n",
    "        self.similarity_matrix=self.my_metric.sim(self.metric,self.pivot)\n",
    "        return self\n",
    "       \n",
    "    '''in the following I am going to define three functions. predict is the function with two entries. one of them is the estimator\n",
    "    (which stores the data about X_train after fitting. the other one is the new data we want to predict.\n",
    "    '''\n",
    "    \n",
    "    def distance (self,F):\n",
    "    \n",
    "       \n",
    "        '''\n",
    "        when we have a data to predict, the similarities between items in this data set should be taken from the similartities we\n",
    "        found  before (in training data). As the knn model is used in the paper so for the target item i we are just looking for\n",
    "        similar items in its defined neighborhood\n",
    "    \n",
    "        '''\n",
    "        \n",
    "        for c in F.columns:\n",
    "            \n",
    "          #here we are jst taking acount those entries similar to the target i which are in the k-neighborhood of the item i.\n",
    "            similarity_matrix_zero=F.copy()\n",
    "            sort_column=similarity_matrix_zero.sort_values(by = [c],ascending=False).index\n",
    "    \n",
    "            index_n=sort_column[0:self.n_neighbors]\n",
    "            \n",
    "            similarity_matrix_zero.loc[list(set(similarity_matrix_zero.index.tolist()).difference (set(index_n.tolist()))),c]=0\n",
    "            \n",
    "            '''Now we have a matrix with non-xero entries juft for those pair in each other neighborhood'''\n",
    "\n",
    "        return similarity_matrix_zero\n",
    "\n",
    "        \n",
    "    def predict(self,data_predict):\n",
    "        \n",
    "  \n",
    "        \n",
    "        '''\n",
    "        To predict a dataset, we have to use one of the prediction_computation models like weighted sum. For that we need two\n",
    "        datasets including pivot and similarity. As it is a test data and we do not have ratings here, we have to use self.pivot \n",
    "        and self.similarity.\n",
    "        \n",
    "        '''\n",
    "        #data_predict_pivot: it should have users in data_predict and all movies\n",
    "        #data_predict_similarity: it should have movies in data_predict and all movies(because we are looking for movies in \n",
    "        #training data  which are similar to movie i in data_predict)\n",
    "        \n",
    "        predicted_users=list(set(data_predict.loc[:,'userId'].values))\n",
    "        all_movies=set((data_predict.loc[:,'movieId'].values)).union (self.pivot.index.values)\n",
    "    \n",
    "        predicted_movies=np.unique(data_predict['movieId'].values).tolist()\n",
    "    \n",
    "        data_predict_pivot=self.pivot.reindex(index=predicted_users,columns=all_movies)\n",
    "        data_predict_similarity=self.similarity_matrix.reindex(index=all_movies,columns=predicted_movies)\n",
    "        '''\n",
    "        Now we have a similairt dataset where each of the predicted movies can be the target and \n",
    "        its similarity with any other movies is considered.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        for i,j in zip(predicted_movies, predicted_movies):\n",
    "            '''\n",
    "            here, we have to put zero each entry of the diameter. Az it is the similairty of an item with itselves and we do not \n",
    "            want to consider it in the neighborhood.\n",
    "            '''\n",
    "            data_predict_similarity.loc[i, j]=0\n",
    "        \n",
    "        data_predict_pivot.iloc[:,:] = np.nan_to_num(data_predict_pivot) \n",
    "        data_predict_similarity.iloc[:,:] = np.nan_to_num(data_predict_similarity)\n",
    "        \n",
    "        \n",
    "        predictions=self.my_predict.pre_model(self.pred,data_predict_pivot,self.distance(data_predict_similarity))\n",
    "        \n",
    "        '''\n",
    "        this is the predicted scores for each predicted user and predicted item in the dataset given to predict function.\n",
    "        \n",
    "        '''\n",
    "        predictions = predictions.reindex(index=predicted_users, columns = predicted_movies)\n",
    "       \n",
    "        y_hat=[]\n",
    "\n",
    "        for i in data_predict.itertuples(index=False):\n",
    "            #i has two component. the first one is the userId and the second one is the movieId\n",
    "            j = i[0]\n",
    "            k = i[1]\n",
    "            y_hat.append(predictions.loc[j,k])\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "\n",
    "    def score(self,X,y):\n",
    "        #in this function we want to calculate the MAE egarding y hat and the real target. So at first, we need to predict X.\n",
    "        predictions = self.predict(X)\n",
    "        predictions = np.nan_to_num(predictions) \n",
    "        MAE = sklearn.metrics.mean_absolute_error(y,predictions)\n",
    "        R_square = r2_score(y,predictions) \n",
    "        return -MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f152c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46883c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
